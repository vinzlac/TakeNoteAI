#!/usr/bin/env python3
"""
Correction IA guid√©e par mots-cl√©s pour les transcriptions/r√©sum√©s (FR).

Id√©e:
- Entr√©e: fichier texte/markdown/JSON + liste de mots-cl√©s m√©tier (fichier .txt ou liste CSV).
- Pour chaque phrase, on identifie les tokens proches des mots-cl√©s (similarit√©) puis on valide avec un mod√®le mask√© FR (CamemBERT):
  si le mot-cl√© propos√© appara√Æt dans le top-k des pr√©dictions, on remplace.
- Produit: fichier corrig√© + rapport des remplacements.

Usage:
  python3 ai_keyword_guided_correction.py input.md --keywords keywords.txt --out output/summaries/input_kw_corrected.md
  python3 ai_keyword_guided_correction.py transcription.json --keywords "Forvia,Azure,meeting" --inplace
"""

import argparse
import json
import re
import difflib
from pathlib import Path
from typing import List, Tuple, Dict, Optional

import torch
from transformers import pipeline


SENTENCE_SPLIT_REGEX = re.compile(r"(?<=[.!?])\s+")
WORD_REGEX = re.compile(r"^[A-Za-z√Ä-√ñ√ò-√∂√∏-√ø'_-]{2,}$")


def read_text(path: Path) -> str:
    raw = path.read_text(encoding='utf-8', errors='ignore')
    if path.suffix.lower() == '.json':
        try:
            data = json.loads(raw)
            raw = (
                data.get('transcription', {}).get('text')
                or data.get('transcription', {}).get('full_text')
                or data.get('text')
                or raw
            )
        except Exception:
            pass
    # retirer blocs code markdown
    raw = re.sub(r"```[\s\S]*?```", " ", raw)
    return raw


def write_text(path: Path, text: str):
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(text, encoding='utf-8')


def split_sentences(text: str) -> List[str]:
    s = SENTENCE_SPLIT_REGEX.split(text.strip())
    return [x.strip() for x in s if x and x.strip()]


def load_keywords(arg: Optional[str]) -> List[str]:
    if not arg:
        return []
    p = Path(arg)
    if p.exists():
        content = p.read_text(encoding='utf-8', errors='ignore')
        if p.suffix.lower() in {'.txt', ''}:
            kws = [l.strip() for l in content.splitlines() if l.strip() and not l.strip().startswith('#')]
            return kws
        else:
            # fallback: split by commas
            return [t.strip() for t in content.split(',') if t.strip()]
    # else treat as csv string
    return [t.strip() for t in arg.split(',') if t.strip()]


def preserve_case(original: str, replacement: str) -> str:
    if original.isupper():
        return replacement.upper()
    if original.istitle():
        return replacement[:1].upper() + replacement[1:]
    return replacement


def best_keyword_candidate(token: str, keywords: List[str], min_ratio: float = 0.50, debug: bool = False) -> Optional[str]:
    """
    Trouve le mot-cl√© le plus similaire au token donn√©.
    Utilise uniquement la similarit√©, pas de mapping pr√©d√©fini.
    """
    token_l = token.lower()
    best: Tuple[str, float] | None = None
    
    for kw in keywords:
        kw_l = kw.lower()
        # Si exact match (insensible casse), retourner directement
        if token_l == kw_l:
            return kw
        # Calcul de similarit√©
        r = difflib.SequenceMatcher(None, token_l, kw_l).ratio()
        if debug:
            print(f"  Debug: '{token_l}' vs '{kw_l}' = {r:.3f}")
        if r >= min_ratio and (best is None or r > best[1]):
            best = (kw, r)
    
    if best and debug:
        print(f"  ‚Üí Meilleur candidat: '{best[0]}' (ratio: {best[1]:.3f})")
    return best[0] if best else None


def lm_confirms(nlp_fill, sentence_tokens: List[str], index: int, candidate: str, topk: int = 5, 
                similarity_score: float = 0.0, debug: bool = False) -> bool:
    masked = sentence_tokens.copy()
    masked[index] = '<mask>'
    masked_sentence = ' '.join(masked)
    try:
        preds = nlp_fill(masked_sentence)
    except Exception:
        return False
    cand_l = candidate.lower()
    if debug:
        print(f"  LM pr√©dictions pour '{masked_sentence}':")
        for i, p in enumerate(preds[:topk], 1):
            print(f"    {i}. {p.get('token_str', '').strip()} (score: {p.get('score', 0):.3f})")
    
    # Chercher dans le top-k √©largi (topk * 2) pour √™tre plus permissif
    extended_topk = min(topk * 2, len(preds))
    for p in preds[:extended_topk]:
        if p.get('token_str', '').strip().lower() == cand_l:
            if debug:
                print(f"  ‚úì '{candidate}' confirm√© par LM (top-{extended_topk})")
            return True
    
    # Si similarit√© raisonnable (>0.50) ET meilleur score LM pas tr√®s √©lev√© (<0.15),
    # accepter quand m√™me (cas o√π transcription erron√©e mais contexte peu clair)
    # ET si le candidat est dans les mots-cl√©s fournis (donc c'est intentionnel)
    if similarity_score > 0.50 and preds and preds[0].get('score', 0) < 0.15:
        if debug:
            print(f"  ‚úì '{candidate}' accept√© (similarit√© {similarity_score:.2f} > 0.50, confiance LM faible {preds[0].get('score', 0):.3f})")
        return True
    
    if debug:
        print(f"  ‚úó '{candidate}' non confirm√© par LM")
    return False


def correct_text_with_keywords(text: str, keywords: List[str], nlp_fill, topk: int = 5, debug: bool = False) -> Tuple[str, List[Dict]]:
    # Pr√©server la structure originale (lignes/paragraphes)
    lines = text.split('\n')
    changes: List[Dict] = []
    corrected_lines: List[str] = []

    for line in lines:
        if not line.strip():
            corrected_lines.append(line)
            continue
        
        # Traiter phrase par phrase dans la ligne
        sentences = split_sentences(line)
        corrected_line_parts = []
        
        for sent in sentences:
            tokens = sent.split()
            for i, tok in enumerate(tokens):
                if not WORD_REGEX.match(tok):
                    continue
                cand = best_keyword_candidate(tok, keywords, debug=debug)
                if not cand:
                    continue
                # Calculer le score de similarit√© pour le passer √† lm_confirms
                tok_l = tok.lower()
                cand_l = cand.lower()
                sim_score = difflib.SequenceMatcher(None, tok_l, cand_l).ratio()
                if debug:
                    print(f"\nüîç Analyse: '{tok}' ‚Üí candidat: '{cand}' (similarit√©: {sim_score:.3f})")
                if lm_confirms(nlp_fill, tokens, i, cand, topk=topk, similarity_score=sim_score, debug=debug):
                    new_tok = preserve_case(tok, cand)
                    if new_tok != tok:
                        if debug:
                            print(f"  ‚úÖ Remplacement: '{tok}' ‚Üí '{new_tok}'")
                        changes.append({'sentence': sent, 'from': tok, 'to': new_tok})
                        tokens[i] = new_tok
            corrected_line_parts.append(' '.join(tokens))
        
        corrected_lines.append(' '.join(corrected_line_parts) if corrected_line_parts else line)

    return ('\n'.join(corrected_lines), changes)


def main() -> int:
    parser = argparse.ArgumentParser(description='Correction IA guid√©e par mots-cl√©s (CamemBERT fill-mask)')
    parser.add_argument('input_file', help='Fichier texte/markdown/JSON √† corriger')
    parser.add_argument('--keywords', help='Fichier .txt de mots-cl√©s (1 par ligne) ou liste CSV')
    parser.add_argument('--topk', type=int, default=5, help='Top-k du mod√®le mask√© pour valider')
    parser.add_argument('--out', help='Chemin du fichier corrig√©')
    parser.add_argument('--inplace', action='store_true', help='Ecrire les corrections dans le fichier d\'entree')
    parser.add_argument('--debug', action='store_true', help='Mode debug (affiche details de la detection/correction)')
    args = parser.parse_args()

    inp = Path(args.input_file)
    if not inp.exists():
        print(f"‚ùå Introuvable: {inp}")
        return 1

    kws = load_keywords(args.keywords)
    if not kws:
        print("‚ö†Ô∏è  Aucun mot-cl√© fourni ‚Äî aucune correction guid√©e ne sera appliqu√©e.")

    text = read_text(inp)

    device = 0 if torch.backends.mps.is_available() else -1
    print("üîÑ Chargement de CamemBERT large (almanach/camembert-large)...")
    try:
        # Utiliser le mod√®le large officiel (335M param√®tres, plus performant)
        nlp_fill = pipeline('fill-mask', model='almanach/camembert-large', device=device)
        print("‚úÖ Mod√®le large charg√© (~335M param√®tres, CCNet 135GB)")
    except Exception as e:
        print(f"‚ö†Ô∏è  Mod√®le large non disponible: {e}")
        print("üîÑ Fallback vers CamemBERT base...")
        nlp_fill = pipeline('fill-mask', model='camembert-base', device=device)
        print("‚úÖ Mod√®le base charg√© (~110M param√®tres)")

    corrected, changes = correct_text_with_keywords(text, kws, nlp_fill, topk=args.topk, debug=args.debug)

    if args.inplace:
        write_text(inp, corrected)
        out_path = inp
    else:
        out_path = Path(args.out) if args.out else Path('output/summaries') / (inp.stem + '_kw_corrected' + inp.suffix)
        write_text(out_path, corrected)

    # Rapport
    report_path = Path('output/analysis') / (inp.stem + '_kw_corrections.json')
    report_path.parent.mkdir(parents=True, exist_ok=True)
    report_path.write_text(json.dumps({'changes_count': len(changes), 'changes': changes}, ensure_ascii=False, indent=2), encoding='utf-8')

    print(f"‚úÖ Corrections appliqu√©es: {len(changes)} remplacements | sortie: {out_path}")
    print(f"üìÑ Rapport: {report_path}")
    return 0


if __name__ == '__main__':
    raise SystemExit(main())



#!/usr/bin/env python3
"""
Script hybride Whisper + pyannote.audio pour transcription avec identification des locuteurs
Combine la transcription de qualit√© de Whisper avec la reconnaissance de locuteurs de pyannote.audio
"""

import os
import sys
import argparse
import json
import tempfile
from pathlib import Path
from datetime import datetime
import whisper
import torch
import torchaudio
from typing import Dict, List, Tuple

# Import pyannote avec gestion d'erreur
try:
    from pyannote.audio import Pipeline
    PYANNOTE_AVAILABLE = True
except ImportError as e:
    print(f"‚ö†Ô∏è  pyannote.audio non disponible: {e}")
    PYANNOTE_AVAILABLE = False

class WhisperSpeakerDiarization:
    """Classe pour combiner Whisper et pyannote.audio."""
    
    def __init__(self, whisper_model="base", device=None):
        """
        Initialise les mod√®les Whisper et pyannote.audio.
        
        Args:
            whisper_model (str): Mod√®le Whisper √† utiliser
            device (str): Device √† utiliser (cuda, mps, cpu)
        """
        self.whisper_model_name = whisper_model
        # Forcer CPU pour √©viter les probl√®mes de compatibilit√© avec pyannote.audio
        self.device = device or "cpu"
        
        print(f"üîß Initialisation des mod√®les...")
        print(f"   Device: {self.device}")
        print(f"   Mod√®le Whisper: {whisper_model}")
        
        # Charger Whisper
        print("üîÑ Chargement du mod√®le Whisper...")
        self.whisper_model = whisper.load_model(whisper_model, device=self.device)
        
        # Charger pyannote.audio (diarisation des locuteurs)
        if PYANNOTE_AVAILABLE:
            print("üîÑ Chargement du mod√®le pyannote.audio...")
            try:
                # D√©sactiver temporairement pyannote.audio √† cause de probl√®mes de compatibilit√©
                # avec PyTorch 2.8.0 et torchcodec
                raise RuntimeError("pyannote.audio temporairement d√©sactiv√© - probl√®mes de compatibilit√©")
                
                self.diarization_pipeline = Pipeline.from_pretrained(
                    "pyannote/speaker-diarization-3.1"
                )
                self.diarization_pipeline.to(torch.device(self.device))
                self.pyannote_available = True
            except Exception as e:
                print(f"‚ö†Ô∏è  pyannote.audio non disponible: {str(e)[:100]}")
                print("üí° Utilisation de la diarisation manuelle optimis√©e √† la place")
                print("üí° Pour de meilleurs r√©sultats, utilisez whisper_balanced_diarization.py")
                self.pyannote_available = False
        else:
            print("üîÑ pyannote.audio non disponible, utilisation de la diarisation manuelle...")
            self.pyannote_available = False
    
    def diarize_speakers(self, audio_path: str) -> List[Dict]:
        """
        Identifie les locuteurs dans l'audio.
        
        Args:
            audio_path (str): Chemin vers le fichier audio
            
        Returns:
            List[Dict]: Liste des segments avec locuteurs
        """
        print("üé§ Identification des locuteurs...")
        
        if self.pyannote_available:
            # Diarisation avec pyannote.audio
            diarization = self.diarization_pipeline(audio_path)
            
            # Convertir en format plus lisible
            speaker_segments = []
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                speaker_segments.append({
                    "start": turn.start,
                    "end": turn.end,
                    "speaker": speaker,
                    "duration": turn.end - turn.start
                })
            
            # Trier par temps de d√©but
            speaker_segments.sort(key=lambda x: x["start"])
            
            print(f"‚úÖ {len(set(seg['speaker'] for seg in speaker_segments))} locuteurs identifi√©s avec pyannote.audio")
            return speaker_segments
        else:
            # Diarisation manuelle bas√©e sur les pauses
            print("üîÑ Utilisation de la diarisation manuelle...")
            return self._manual_diarization(audio_path)
    
    def _manual_diarization(self, audio_path: str) -> List[Dict]:
        """
        Diarisation manuelle bas√©e sur les pauses et patterns.
        
        Args:
            audio_path (str): Chemin vers le fichier audio
            
        Returns:
            List[Dict]: Segments avec locuteurs assign√©s
        """
        # D'abord, transcrir avec Whisper pour obtenir les segments
        result = self.whisper_model.transcribe(audio_path, verbose=False)
        segments = result.get("segments", [])
        
        if not segments:
            return []
        
        speaker_segments = []
        current_speaker = "SPEAKER_00"
        speaker_count = 1
        
        for i, segment in enumerate(segments):
            seg_start = segment["start"]
            seg_end = segment["end"]
            
            # Calculer la pause avant ce segment
            pause_duration = seg_start - segments[i-1]["end"] if i > 0 else 0
            
            # R√®gles pour changer de locuteur
            speaker_changed = False
            
            # 1. Pause significative
            if pause_duration > 2.0:
                speaker_changed = True
                print(f"   üîÑ Changement par pause √† {seg_start:.1f}s (pause: {pause_duration:.1f}s)")
            
            # 2. R√©ponses courtes
            elif (len(segment["text"]) < 20 and 
                  pause_duration > 0.8 and
                  any(word in segment["text"].lower() for word in ["oui", "non", "ok", "d'accord"])):
                speaker_changed = True
                print(f"   üîÑ Changement par r√©ponse courte √† {seg_start:.1f}s")
            
            # Appliquer le changement
            if speaker_changed and i > 0:
                if current_speaker == "SPEAKER_00":
                    current_speaker = "SPEAKER_01"
                    speaker_count = max(speaker_count, 2)
                else:
                    current_speaker = "SPEAKER_00"
            
            speaker_segments.append({
                "start": seg_start,
                "end": seg_end,
                "speaker": current_speaker,
                "duration": seg_end - seg_start
            })
        
        print(f"‚úÖ {len(set(seg['speaker'] for seg in speaker_segments))} locuteurs identifi√©s (m√©thode manuelle)")
        return speaker_segments
    
    def transcribe_audio(self, audio_path: str, language=None) -> Dict:
        """
        Transcrit l'audio avec Whisper.
        
        Args:
            audio_path (str): Chemin vers le fichier audio
            language (str): Langue pour la transcription
            
        Returns:
            Dict: R√©sultat de la transcription Whisper
        """
        print("üé§ Transcription avec Whisper...")
        
        result = self.whisper_model.transcribe(
            audio_path,
            language=language,
            verbose=True
        )
        
        return result
    
    def align_speakers_with_transcription(self, speaker_segments: List[Dict], 
                                        whisper_result: Dict) -> List[Dict]:
        """
        Aligne les segments de locuteurs avec la transcription Whisper.
        
        Args:
            speaker_segments (List[Dict]): Segments des locuteurs
            whisper_result (Dict): R√©sultat de Whisper
            
        Returns:
            List[Dict]: Transcription align√©e avec les locuteurs
        """
        print("üîó Alignement des locuteurs avec la transcription...")
        
        aligned_segments = []
        whisper_segments = whisper_result.get("segments", [])
        
        for whisper_seg in whisper_segments:
            seg_start = whisper_seg["start"]
            seg_end = whisper_seg["end"]
            seg_text = whisper_seg["text"].strip()
            
            # Trouver le locuteur dominant pour ce segment
            dominant_speaker = self._find_dominant_speaker(speaker_segments, seg_start, seg_end)
            
            aligned_segments.append({
                "start": seg_start,
                "end": seg_end,
                "text": seg_text,
                "speaker": dominant_speaker,
                "confidence": whisper_seg.get("avg_logprob", 0)
            })
        
        return aligned_segments
    
    def _find_dominant_speaker(self, speaker_segments: List[Dict], 
                             start_time: float, end_time: float) -> str:
        """
        Trouve le locuteur dominant pour un segment donn√©.
        
        Args:
            speaker_segments (List[Dict]): Segments des locuteurs
            start_time (float): Temps de d√©but
            end_time (float): Temps de fin
            
        Returns:
            str: Nom du locuteur dominant
        """
        # Calculer le temps de parole de chaque locuteur dans le segment
        speaker_times = {}
        
        for seg in speaker_segments:
            # Calculer l'intersection entre le segment de locuteur et le segment de transcription
            overlap_start = max(seg["start"], start_time)
            overlap_end = min(seg["end"], end_time)
            
            if overlap_start < overlap_end:
                overlap_duration = overlap_end - overlap_start
                speaker = seg["speaker"]
                speaker_times[speaker] = speaker_times.get(speaker, 0) + overlap_duration
        
        # Retourner le locuteur avec le plus de temps de parole
        if speaker_times:
            return max(speaker_times, key=speaker_times.get)
        else:
            return "SPEAKER_00"  # Locuteur par d√©faut
    
    def process_audio(self, audio_path: str, output_path: str = None, 
                     language: str = None, output_format: str = "txt") -> str:
        """
        Traite un fichier audio complet : diarisation + transcription.
        
        Args:
            audio_path (str): Chemin vers le fichier audio
            output_path (str): Chemin de sortie
            language (str): Langue pour la transcription
            output_format (str): Format de sortie (txt, json, srt)
            
        Returns:
            str: Chemin du fichier de sortie
        """
        audio_path = Path(audio_path)
        
        if not audio_path.exists():
            raise FileNotFoundError(f"Le fichier {audio_path} n'existe pas")
        
        # G√©n√©rer le nom de sortie si non fourni
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = audio_path.parent / f"{audio_path.stem}_speakers_{timestamp}.{output_format}"
        else:
            output_path = Path(output_path)
        
        print(f"üöÄ Traitement: {audio_path.name}")
        print(f"   Taille: {audio_path.stat().st_size / 1024 / 1024:.2f} MB")
        
        try:
            # √âtape 1: Diarisation des locuteurs
            speaker_segments = self.diarize_speakers(str(audio_path))
            
            # √âtape 2: Transcription avec Whisper
            whisper_result = self.transcribe_audio(str(audio_path), language)
            
            # √âtape 3: Alignement des locuteurs avec la transcription
            aligned_segments = self.align_speakers_with_transcription(
                speaker_segments, whisper_result
            )
            
            # √âtape 4: Sauvegarde selon le format
            if output_format == "txt":
                self._save_as_text(aligned_segments, output_path)
            elif output_format == "json":
                self._save_as_json(aligned_segments, whisper_result, output_path)
            elif output_format == "srt":
                self._save_as_srt(aligned_segments, output_path)
            else:
                raise ValueError(f"Format non support√©: {output_format}")
            
            # Statistiques
            speakers = list(set(seg["speaker"] for seg in aligned_segments))
            total_duration = max(seg["end"] for seg in aligned_segments)
            
            print(f"\n‚úÖ Traitement termin√©!")
            print(f"üìä Statistiques:")
            print(f"   - Dur√©e totale: {total_duration:.2f} secondes")
            print(f"   - Nombre de locuteurs: {len(speakers)}")
            print(f"   - Locuteurs: {', '.join(speakers)}")
            print(f"   - Langue d√©tect√©e: {whisper_result.get('language', 'Inconnue')}")
            print(f"   - Fichier de sortie: {output_path}")
            
            return str(output_path)
            
        except Exception as e:
            print(f"‚ùå Erreur lors du traitement: {e}")
            raise
    
    def _save_as_text(self, aligned_segments: List[Dict], output_path: Path):
        """Sauvegarde en format texte avec locuteurs."""
        with open(output_path, 'w', encoding='utf-8') as f:
            for seg in aligned_segments:
                start_time = self._format_time(seg["start"])
                end_time = self._format_time(seg["end"])
                f.write(f"[{start_time} - {end_time}] {seg['speaker']}: {seg['text']}\n\n")
    
    def _save_as_json(self, aligned_segments: List[Dict], whisper_result: Dict, output_path: Path):
        """Sauvegarde en format JSON."""
        result = {
            "metadata": {
                "language": whisper_result.get("language"),
                "duration": whisper_result.get("duration"),
                "speakers": list(set(seg["speaker"] for seg in aligned_segments))
            },
            "segments": aligned_segments
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
    
    def _save_as_srt(self, aligned_segments: List[Dict], output_path: Path):
        """Sauvegarde en format SRT avec locuteurs."""
        with open(output_path, 'w', encoding='utf-8') as f:
            for i, seg in enumerate(aligned_segments, 1):
                start_time = self._format_time_srt(seg["start"])
                end_time = self._format_time_srt(seg["end"])
                text = f"{seg['speaker']}: {seg['text']}"
                
                f.write(f"{i}\n")
                f.write(f"{start_time} --> {end_time}\n")
                f.write(f"{text}\n\n")
    
    def _format_time(self, seconds: float) -> str:
        """Formate le temps en HH:MM:SS."""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        return f"{hours:02d}:{minutes:02d}:{secs:02d}"
    
    def _format_time_srt(self, seconds: float) -> str:
        """Formate le temps pour SRT (HH:MM:SS,mmm)."""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millis = int((seconds % 1) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"


def main():
    """Fonction principale."""
    parser = argparse.ArgumentParser(
        description="Transcription avec identification des locuteurs (Whisper + pyannote.audio)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation:
  %(prog)s audio.mp3                           # Transcription avec locuteurs
  %(prog)s audio.mp3 -m large -l fr           # Mod√®le large, fran√ßais
  %(prog)s audio.mp3 -f json                  # Format JSON
  %(prog)s audio.mp3 -f srt                   # Format SRT (sous-titres)
        """
    )
    
    parser.add_argument("input", help="Fichier audio d'entr√©e")
    parser.add_argument("-o", "--output", help="Fichier de sortie (optionnel)")
    parser.add_argument("-m", "--model", choices=["tiny", "base", "small", "medium", "large"], 
                       default="base", help="Mod√®le Whisper √† utiliser (d√©faut: base)")
    parser.add_argument("-l", "--language", help="Code langue (ex: fr, en). D√©tection automatique si non sp√©cifi√©")
    parser.add_argument("-f", "--format", choices=["txt", "json", "srt"], 
                       default="txt", help="Format de sortie (d√©faut: txt)")
    parser.add_argument("--device", choices=["cpu", "cuda", "mps"], 
                       help="Device √† utiliser (auto-d√©tection par d√©faut)")
    
    args = parser.parse_args()
    
    try:
        # Initialiser le processeur
        processor = WhisperSpeakerDiarization(
            whisper_model=args.model,
            device=args.device
        )
        
        # Traiter l'audio
        output_file = processor.process_audio(
            args.input,
            args.output,
            args.language,
            args.format
        )
        
        print(f"\nüéâ Succ√®s! Transcription avec locuteurs sauvegard√©e: {output_file}")
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Traitement interrompu par l'utilisateur")
        sys.exit(1)
    except Exception as e:
        print(f"\nüí• Erreur fatale: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()

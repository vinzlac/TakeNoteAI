#!/usr/bin/env python3
"""
Script pour extraire les mots-cl√©s r√©els des documents dans ChromaDB avec KeyBERT
"""

import chromadb
import json
from collections import Counter
from pathlib import Path

def extract_keywords_with_keybert():
    """Extrait les mots-cl√©s avec KeyBERT des documents dans ChromaDB."""
    try:
        # Import KeyBERT
        from keybert import KeyBERT
        from sentence_transformers import SentenceTransformer
        
        print("üîÑ Initialisation de KeyBERT...")
        model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        kw_model = KeyBERT(model=model)
        
        # Connexion √† ChromaDB
        client = chromadb.PersistentClient(path="./chroma_db")
        collection = client.get_collection("transcriptions")
        
        # R√©cup√©rer tous les documents
        results = collection.get(include=['documents', 'metadatas'])
        
        if not results['ids']:
            print("‚ùå Aucun document trouv√© dans la base")
            return
        
        print(f"üìä Analyse de {len(results['ids'])} documents dans ChromaDB")
        print("=" * 60)
        
        all_keywords = []
        document_keywords = {}
        
        for i, doc_id in enumerate(results['ids']):
            metadata = results['metadatas'][i]
            document = results['documents'][i]
            
            print(f"\nüìÑ Document {i+1}: {metadata.get('filename', 'N/A')}")
            print(f"   ID: {doc_id}")
            print(f"   Dur√©e: {metadata.get('duration', 0):.1f}s")
            print(f"   Langue: {metadata.get('language', 'N/A')}")
            print(f"   M√©thode: {metadata.get('transcription_method', 'N/A')}")
            
            # Extraire les mots-cl√©s avec KeyBERT
            print("   üîç Extraction des mots-cl√©s avec KeyBERT...")
            keywords = kw_model.extract_keywords(
                document,
                keyphrase_ngram_range=(1, 3),
                stop_words='french',
                top_n=15,
                use_mmr=True,
                diversity=0.5
            )
            
            # Filtrer les mots-cl√©s m√©tiers
            business_keywords = filter_business_keywords(keywords, document)
            document_keywords[doc_id] = business_keywords
            
            if business_keywords:
                print(f"   üîë Mots-cl√©s m√©tiers d√©tect√©s ({len(business_keywords)}):")
                for j, (keyword, score) in enumerate(business_keywords, 1):
                    print(f"      {j:2d}. {keyword:<30} (score: {score:.3f})")
                all_keywords.extend([kw[0] for kw in business_keywords])
            else:
                print("   ‚ùå Aucun mot-cl√© m√©tier d√©tect√©")
        
        # Analyse globale
        print("\n" + "=" * 60)
        print("üîç ANALYSE GLOBALE DES MOTS-CL√âS M√âTIERS")
        print("=" * 60)
        
        if all_keywords:
            # Compter les occurrences
            keyword_counts = Counter(all_keywords)
            
            print(f"üìä Statistiques:")
            print(f"   - Total mots-cl√©s uniques: {len(keyword_counts)}")
            print(f"   - Total occurrences: {sum(keyword_counts.values())}")
            print(f"   - Moyenne par document: {sum(keyword_counts.values()) / len(results['ids']):.1f}")
            
            print(f"\nüèÜ Top 20 des mots-cl√©s m√©tiers les plus fr√©quents:")
            for i, (keyword, count) in enumerate(keyword_counts.most_common(20), 1):
                print(f"   {i:2d}. {keyword:<35} ({count} fois)")
            
            # Sauvegarder les r√©sultats
            output_data = {
                "total_documents": len(results['ids']),
                "total_unique_keywords": len(keyword_counts),
                "total_occurrences": sum(keyword_counts.values()),
                "top_keywords": dict(keyword_counts.most_common(50)),
                "document_keywords": document_keywords,
                "extraction_method": "KeyBERT + filtrage m√©tier"
            }
            
            output_file = "keywords_from_embeddings.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            print(f"\n‚úÖ Analyse sauvegard√©e: {output_file}")
            
        else:
            print("‚ùå Aucun mot-cl√© m√©tier trouv√© dans les documents")
        
    except ImportError:
        print("‚ùå KeyBERT non install√©. Installez avec: pip install keybert")
    except Exception as e:
        print(f"‚ùå Erreur: {e}")

def filter_business_keywords(keywords, text):
    """Filtre les mots-cl√©s pour ne garder que ceux li√©s au m√©tier."""
    # Mots-cl√©s m√©tiers typiques
    business_indicators = [
        "projet", "√©quipe", "d√©veloppement", "architecture", "code", "review",
        "maintenance", "go-live", "application", "syst√®me", "technologie",
        "processus", "m√©thodologie", "qualit√©", "performance", "s√©curit√©",
        "int√©gration", "d√©ploiement", "test", "validation", "documentation",
        "formation", "support", "monitoring", "backup", "migration",
        "analyse", "conception", "impl√©mentation", "optimisation", "r√©solution",
        "planification", "gestion", "coordination", "collaboration", "communication",
        "r√©union", "pr√©sentation", "formation", "audit", "compliance",
        "budget", "ressources", "planning", "livrable", "milestone",
        "client", "utilisateur", "interface", "base de donn√©es", "serveur",
        "r√©seau", "cloud", "mobile", "web", "api", "framework", "library"
    ]
    
    filtered_keywords = []
    text_lower = text.lower()
    
    for keyword, score in keywords:
        keyword_lower = keyword.lower()
        
        # V√©rifier si le mot-cl√© contient des indicateurs m√©tiers
        is_business = any(indicator in keyword_lower for indicator in business_indicators)
        
        # V√©rifier la fr√©quence dans le texte
        frequency = text_lower.count(keyword_lower)
        
        # Score combin√©: score KeyBERT + fr√©quence + indicateur m√©tier
        combined_score = score + (frequency * 0.1) + (0.2 if is_business else 0)
        
        if combined_score > 0.3:  # Seuil minimum
            filtered_keywords.append((keyword, combined_score))
    
    # Trier par score d√©croissant
    filtered_keywords.sort(key=lambda x: x[1], reverse=True)
    
    return filtered_keywords[:10]  # Top 10

if __name__ == "__main__":
    extract_keywords_with_keybert()

